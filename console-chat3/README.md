
Jumpyter --> <http://localhost:8888/tree>

```bash
 # Create a new conda environment with Python 3.13.5
 conda create -n bootagents python=3.13.5

 # Activate the environment
 conda activate bootagent

 # Install poetry from conda-forge
 conda install -c conda-forge poetry
 
 # guardo el ambiente par ala proxima en otro pc 
 # si no ponemos --from-history te emete todas las deps juntas con proetry
 conda env export --from-history > environment.yml
```

Lo demas que se instale lo maneja poetry como gestor de dependencias

# Create a new environment from the YAML file Esto si ya estoy en otro pc

```bash
 conda env create -f environment.yml
```

- Create pyproject.toml

```bash
    poetry init
```

 If Error:
  check name = "app"
  readme file exxist
  app folder withi main.py exist
  
# reparar el toml

 Para que se puedan hacer instalaciones langchaing y salen errores
    For langchain-text-splitters, a possible solution would be to set the `python` property to ">=3.13,<4.0"

 ```bash
  requires-python = ">=3.10,<4.0"    # â† aquÃ­ ajustas el rango de Python
 ```

# regullary update with

```bash
 conda env update -f environment.yml
```

ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹
                            Installing dependencies
ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹

âœ… poetry add langchain langchain-community
âœ… poetry add dotenv pendulum
âœ… poetry add faiss-cpu  

  âŒ poetry add sentence-transformers
âœ… conda install -c conda-forge sentence-transformers
âŒ conda install -c conda-forge huggingface-hub
âœ… poetry add huggingface-hub
âœ… poetry add langchain-core langchain-ollama
âœ… poetry add langchain-huggingface

ğŸ“Œfalta pero con docker llama no hace falta
 poetry add llama-cpp-python
     Â¿CuÃ¡ndo usar llama-cpp-python?
        Lo que querÃ©s hacer Â¿UsÃ¡s llama-cpp-python?
        Usar modelos GGUF sin Docker ni servidor âœ… SÃ­
        Integrar con LangChain sin levantar Ollama âœ… SÃ­
        Usar LangChain con Ollama y Docker âŒ No hace falta

 conda install -c conda-forge llama-cpp-python -y

Â¿Por quÃ© usar Poetry si ya tienes conda y pip?
    Consistencia de versiones: pip por sÃ­ solo no crea un lock file (salvo que uses pip freeze > requirements.txt,
    pero eso no captura bien los subâ€“dependencias). Poetry automatiza este proceso de lock.
    Reproducibilidad: Si en tu equipo o CI alguien corre poetry install, obtiene el mismo entorno exacto de versiones.

Â¿No se arma un quilombo mezclÃ¡ndolos?
    Puede haber conflictos si instalas cosas al voleo con conda install dentro del virtualenv que Poetry creÃ³,
    o si combinas pip install y poetry add sin cuidado. Para evitarlo:

  1-  SeleccionÃ¡ una sola capa para tu proyecto:

        OpciÃ³n A: Todo con Conda.

            - conda create -n proyecto python=3.x

       UsÃ¡s conda install y, en Ãºltima instancia, pip install para paquetes que no existan en conda-forge.

ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹
                            Ollama Contrainer
ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹
Chat <https://chatgpt.com/c/6874fa6b-f334-8002-bb53-d764816185e5>

ğŸš« Antes hay que descargarce Ollama <https://ollama.com/download>

Hay que descargar el modelo dentro del contenedor

âœ… Resumen tÃ©cnico
El contenedor ollama/ollama expone el servicio en <http://localhost:11434>
LangChain se conecta automÃ¡ticamente a ese endpoint
  Ollama(model="llama3.2:1B-Q4_0")
  Si no estÃ¡ descargado, da error hasta que hagas:
  
ProbÃ¡ desde terminal (modo curl):

1) revisar docker-compose.yml para ver el nombre del contenedor
2) Levantar el container
    docker compose up -d
3) entro en el contenedor
 docker exec -it llama3 bash

  ğŸ³ Descargar el modelo dentro del contenedor
  
    ollama pull llama3           modelos como llama3
    ollama pull llama3.2:1b      modelos como llama3.2:1B-Q4_0

  âœ… Â¿QuÃ© pasa si se apaga el contenedor?

  El modelo no se borra.
   Docker guarda ese volumen (ollama-data) en el host.

  âœ… desde dentro del contenedor, el modelo se descarga y se guarda en:

    /root/.ollama

    volumes:
      - ollama-data:/root/.ollama
