

# Usamos el modelo chico para demos
# llm = Ollama(model="llama3.2:1B-Q4_0")


# pregunta = "¬øQu√© es un endpoint REST en programaci√≥n?"
# respuesta = llm.invoke(pregunta)

# print(f"üß† Pregunta: {pregunta}")
# print(f"ü§ñ Respuesta: {respuesta}")
from pathlib import Path
import traceback

from app.boot_llama import ChatBootLlama

print("Iniciando start...")
def main():
    try:
        chat:ChatBootLlama = ChatBootLlama()
        chat.run_chat()
    except Exception as e:
        
        print(f"‚ö†Ô∏è Error atrapado: {e}")
        traceback.print_exc()
 


if __name__ == "__main__": 
    main()
